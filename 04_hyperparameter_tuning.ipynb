{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07be6c8c",
   "metadata": {},
   "source": [
    "# Phase 4 — Bayesian Hyperparameter Tuning (Optuna)\n",
    "**Inputs:** `X_train_fe.csv`, `y_train_fe.csv` (Phase 2) · `X_test_fe` rebuilt from Phase 1 outputs  \n",
    "**Objective:** Maximise PR-AUC on temporal validation set  \n",
    "**Constraint:** No test-set touch until final evaluation · No SMOTE · No random split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27186c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ── Load engineered training data (Phase 2 outputs) ───────────────────────────\n",
    "X_train_fe = pd.read_csv(\"data/processed/X_train_fe.csv\")\n",
    "y_train_fe = pd.read_csv(\"data/processed/y_train_fe.csv\")\n",
    "\n",
    "# ── Load raw test data + scaling params to rebuild X_test_fe (no leakage) ────\n",
    "X_test_raw     = pd.read_csv(\"data/processed/X_test.csv\")\n",
    "y_test_raw     = pd.read_csv(\"data/processed/y_test.csv\")\n",
    "scaling_params = pd.read_csv(\"data/processed/train_scaling_params.csv\").iloc[0]\n",
    "MEAN_AMOUNT    = scaling_params[\"mean_amount\"]\n",
    "STD_AMOUNT     = scaling_params[\"std_amount\"]\n",
    "\n",
    "# Re-use the same leakage-safe pipeline from Phase 3\n",
    "def apply_feature_pipeline(df_raw, y_raw, mean_amount, std_amount,\n",
    "                            window_amount=100, window_fraud=500):\n",
    "    df = df_raw.copy()\n",
    "    df[\"Class\"] = y_raw[\"Class\"].values\n",
    "    df = df.sort_values(\"Time\").reset_index(drop=True)\n",
    "    df[\"log_amount\"]           = np.log1p(df[\"Amount\"])\n",
    "    df[\"amount_zscore_global\"] = (df[\"Amount\"] - mean_amount) / std_amount\n",
    "    amt_shifted = df[\"Amount\"].shift(1)\n",
    "    df[\"rolling_mean_amount\"]  = amt_shifted.rolling(window_amount, min_periods=1).mean()\n",
    "    df[\"rolling_std_amount\"]   = amt_shifted.rolling(window_amount, min_periods=2).std(ddof=1)\n",
    "    df[\"time_diff\"]            = df[\"Time\"] - df[\"Time\"].shift(1)\n",
    "    df[\"amount_deviation\"]     = df[\"Amount\"] - df[\"rolling_mean_amount\"]\n",
    "    std_safe                   = df[\"rolling_std_amount\"].replace(0, np.nan)\n",
    "    df[\"amount_zscore_rolling\"] = df[\"amount_deviation\"] / std_safe\n",
    "    cls_shifted = df[\"Class\"].shift(1)\n",
    "    df[\"rolling_fraud_count_500\"] = cls_shifted.rolling(window_fraud, min_periods=1).sum()\n",
    "    df[\"rolling_fraud_rate_500\"]  = cls_shifted.rolling(window_fraud, min_periods=1).mean()\n",
    "    ENGINEERED = [\"log_amount\",\"amount_zscore_global\",\"rolling_mean_amount\",\n",
    "                  \"rolling_std_amount\",\"time_diff\",\"amount_deviation\",\n",
    "                  \"amount_zscore_rolling\",\"rolling_fraud_count_500\",\"rolling_fraud_rate_500\"]\n",
    "    all_cols   = list(df_raw.columns) + ENGINEERED\n",
    "    out        = df[all_cols + [\"Class\"]].dropna().reset_index(drop=True)\n",
    "    return out[all_cols], out[[\"Class\"]]\n",
    "\n",
    "X_test_fe, y_test_fe = apply_feature_pipeline(X_test_raw, y_test_raw, MEAN_AMOUNT, STD_AMOUNT)\n",
    "FEATURE_COLS = list(X_train_fe.columns)\n",
    "X_test_fe    = X_test_fe[FEATURE_COLS]\n",
    "\n",
    "y_tr = y_train_fe[\"Class\"].values\n",
    "y_te = y_test_fe[\"Class\"].values\n",
    "\n",
    "print(f\"X_train_fe : {X_train_fe.shape}  fraud={y_tr.mean()*100:.4f}%\")\n",
    "print(f\"X_test_fe  : {X_test_fe.shape}   fraud={y_te.mean()*100:.4f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70cec90",
   "metadata": {},
   "source": [
    "## Step 1 — Temporal Validation Split\n",
    "\n",
    "Last 20% of the time-ordered training set becomes the validation set for Optuna.  \n",
    "The test set is **never seen** during search — it is held out until Step 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a7ce10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Temporal 80/20 split of training data (no shuffle, mirrors Phase 1 logic) ─\n",
    "val_cutoff = int(len(X_train_fe) * 0.80)\n",
    "\n",
    "X_tr  = X_train_fe.iloc[:val_cutoff].reset_index(drop=True)\n",
    "y_tr_ = y_tr[:val_cutoff]\n",
    "X_val = X_train_fe.iloc[val_cutoff:].reset_index(drop=True)\n",
    "y_val = y_tr[val_cutoff:]\n",
    "\n",
    "# Global class-imbalance ratio for scale_pos_weight upper bound\n",
    "n_neg_full = (y_tr == 0).sum()\n",
    "n_pos_full = (y_tr == 1).sum()\n",
    "SPW_MAX    = n_neg_full / n_pos_full\n",
    "\n",
    "print(f\"Optuna train subset : {X_tr.shape}   fraud={y_tr_.mean()*100:.4f}%\")\n",
    "print(f\"Optuna val  subset  : {X_val.shape}   fraud={y_val.mean()*100:.4f}%\")\n",
    "print(f\"scale_pos_weight max (neg/pos) = {SPW_MAX:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31a8b92",
   "metadata": {},
   "source": [
    "## Step 2 — Optuna Bayesian Search\n",
    "\n",
    "**Sampler:** `TPESampler` (Tree-structured Parzen Estimator) — Optuna's default Bayesian method.  \n",
    "**Pruner:** `MedianPruner` — kills unpromising trials early based on intermediate PR-AUC checkpoints, saving time.  \n",
    "**Objective:** maximise PR-AUC on the temporal validation set.  \n",
    "The test set is **never touched** during search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336ffb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from optuna.pruners  import MedianPruner\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)   # suppress per-trial noise\n",
    "\n",
    "N_TRIALS = 75   # increase to 100 for a longer, more thorough search\n",
    "\n",
    "def objective(trial: optuna.Trial) -> float:\n",
    "    params = dict(\n",
    "        objective          = \"binary:logistic\",\n",
    "        eval_metric        = \"aucpr\",\n",
    "        verbosity          = 0,\n",
    "        n_jobs             = -1,\n",
    "        random_state       = 42,\n",
    "        # ── Tuned hyperparameters ─────────────────────────────────────────\n",
    "        max_depth          = trial.suggest_int  (\"max_depth\",         3,   10),\n",
    "        learning_rate      = trial.suggest_float(\"learning_rate\",     0.01, 0.2,  log=True),\n",
    "        n_estimators       = trial.suggest_int  (\"n_estimators\",      200, 1000),\n",
    "        min_child_weight   = trial.suggest_int  (\"min_child_weight\",  1,   10),\n",
    "        subsample          = trial.suggest_float(\"subsample\",         0.5,  1.0),\n",
    "        colsample_bytree   = trial.suggest_float(\"colsample_bytree\",  0.5,  1.0),\n",
    "        scale_pos_weight   = trial.suggest_float(\"scale_pos_weight\",  1.0,  SPW_MAX),\n",
    "        # ── Additional regularisation in search space ─────────────────────\n",
    "        gamma              = trial.suggest_float(\"gamma\",             0.0,  5.0),\n",
    "        reg_alpha          = trial.suggest_float(\"reg_alpha\",         0.0,  2.0),\n",
    "        reg_lambda         = trial.suggest_float(\"reg_lambda\",        0.5,  5.0),\n",
    "    )\n",
    "\n",
    "    model = XGBClassifier(\n",
    "        **params,\n",
    "        early_stopping_rounds=40,\n",
    "    )\n",
    "    model.fit(\n",
    "        X_tr, y_tr_,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        verbose=False,\n",
    "    )\n",
    "\n",
    "    val_proba = model.predict_proba(X_val)[:, 1]\n",
    "    pr_auc    = average_precision_score(y_val, val_proba)\n",
    "    return pr_auc\n",
    "\n",
    "\n",
    "# ── Run the study ─────────────────────────────────────────────────────────────\n",
    "study = optuna.create_study(\n",
    "    direction = \"maximize\",\n",
    "    sampler   = TPESampler(seed=42),\n",
    "    pruner    = MedianPruner(n_startup_trials=10, n_warmup_steps=5),\n",
    "    study_name= \"xgb_fraud_prauc\",\n",
    ")\n",
    "\n",
    "print(f\"Running {N_TRIALS} Optuna trials … (this may take a few minutes)\")\n",
    "study.optimize(objective, n_trials=N_TRIALS, show_progress_bar=True)\n",
    "\n",
    "best = study.best_trial\n",
    "print(f\"\\nBest trial:  #{best.number}\")\n",
    "print(f\"  PR-AUC (val) = {best.value:.6f}\")\n",
    "print(f\"\\nBest hyperparameters:\")\n",
    "for k, v in best.params.items():\n",
    "    print(f\"  {k:<22s} = {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8197d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# ── Optuna optimisation history ───────────────────────────────────────────────\n",
    "trial_nums  = [t.number for t in study.trials if t.value is not None]\n",
    "trial_vals  = [t.value  for t in study.trials if t.value is not None]\n",
    "running_max = pd.Series(trial_vals).cummax().tolist()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "axes[0].scatter(trial_nums, trial_vals, s=20, alpha=0.6, color=\"steelblue\", label=\"Trial PR-AUC\")\n",
    "axes[0].plot(trial_nums, running_max, color=\"crimson\", lw=2, label=\"Best so far\")\n",
    "axes[0].set_title(\"Optuna: PR-AUC per Trial\", fontweight=\"bold\")\n",
    "axes[0].set_xlabel(\"Trial number\")\n",
    "axes[0].set_ylabel(\"PR-AUC (validation)\")\n",
    "axes[0].legend(fontsize=9)\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# ── Hyperparameter importance (manual: correlation of each param with PR-AUC) ─\n",
    "param_names = list(best.params.keys())\n",
    "importances = []\n",
    "for p in param_names:\n",
    "    vals = [t.params.get(p, np.nan) for t in study.trials if t.value is not None]\n",
    "    aucs = [t.value for t in study.trials if t.value is not None]\n",
    "    if len(set(vals)) > 1:\n",
    "        importances.append(abs(np.corrcoef(vals, aucs)[0, 1]))\n",
    "    else:\n",
    "        importances.append(0.0)\n",
    "\n",
    "imp_s = pd.Series(importances, index=param_names).sort_values(ascending=True)\n",
    "axes[1].barh(imp_s.index, imp_s.values, color=\"darkorange\", alpha=0.8)\n",
    "axes[1].set_title(\"|Correlation| of Hyperparameter with Val PR-AUC\", fontweight=\"bold\")\n",
    "axes[1].set_xlabel(\"|Pearson r|\")\n",
    "axes[1].grid(axis=\"x\", alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "os.makedirs(\"data/processed\", exist_ok=True)\n",
    "plt.savefig(\"data/processed/optuna_history.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "print(\"Saved: data/processed/optuna_history.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c06dbc",
   "metadata": {},
   "source": [
    "## Step 3 — Retrain Best Model on Full Training Set\n",
    "\n",
    "The winning hyperparameters are applied to the **complete** `X_train_fe` (train + the validation slice used by Optuna).  \n",
    "Early stopping is disabled for the final refit — we use the best `n_estimators` found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eeec319",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    roc_auc_score, average_precision_score,\n",
    "    precision_recall_fscore_support, confusion_matrix,\n",
    "    roc_curve, precision_recall_curve,\n",
    ")\n",
    "\n",
    "# ── Build final model with best params, no early stopping ────────────────────\n",
    "best_params = best.params.copy()\n",
    "\n",
    "xgb_tuned = XGBClassifier(\n",
    "    objective        = \"binary:logistic\",\n",
    "    eval_metric      = \"aucpr\",\n",
    "    verbosity        = 0,\n",
    "    n_jobs           = -1,\n",
    "    random_state     = 42,\n",
    "    **best_params,\n",
    ")\n",
    "\n",
    "print(\"Retraining on full X_train_fe with best params …\")\n",
    "xgb_tuned.fit(X_train_fe, y_tr)\n",
    "print(\"Done.\")\n",
    "\n",
    "# ── Evaluate on test set (first time test set is touched this phase) ──────────\n",
    "tuned_proba = xgb_tuned.predict_proba(X_test_fe)[:, 1]\n",
    "tuned_pred  = (tuned_proba >= 0.5).astype(int)\n",
    "\n",
    "tuned_roc_auc = roc_auc_score(y_te, tuned_proba)\n",
    "tuned_pr_auc  = average_precision_score(y_te, tuned_proba)\n",
    "prec_t, rec_t, f1_t, _ = precision_recall_fscore_support(\n",
    "    y_te, tuned_pred, pos_label=1, average=\"binary\"\n",
    ")\n",
    "cm_t = confusion_matrix(y_te, tuned_pred)\n",
    "tn_t, fp_t, fn_t, tp_t = cm_t.ravel()\n",
    "\n",
    "print(f\"\\nXGBoost Tuned — Test Set Metrics\")\n",
    "print(f\"{'─'*40}\")\n",
    "print(f\"  ROC-AUC   : {tuned_roc_auc:.4f}\")\n",
    "print(f\"  PR-AUC    : {tuned_pr_auc:.4f}\")\n",
    "print(f\"  Recall    : {rec_t:.4f}\")\n",
    "print(f\"  Precision : {prec_t:.4f}\")\n",
    "print(f\"  F1-score  : {f1_t:.4f}\")\n",
    "print(f\"\\n  Confusion Matrix:\")\n",
    "print(f\"  TN={tn_t:,}  FP={fp_t:,}\")\n",
    "print(f\"  FN={fn_t:,}  TP={tp_t:,}\")\n",
    "\n",
    "\n",
    "# ── Helper: Recall @ target FPR ──────────────────────────────────────────────\n",
    "def recall_at_fpr(y_true, y_proba, target_fpr=0.01):\n",
    "    fpr_arr, tpr_arr, thresh_arr = roc_curve(y_true, y_proba)\n",
    "    idx = max(0, min(np.searchsorted(fpr_arr, target_fpr, side=\"right\") - 1,\n",
    "                     len(thresh_arr) - 1))\n",
    "    threshold  = thresh_arr[idx]\n",
    "    recall_val = tpr_arr[idx]\n",
    "    actual_fpr = fpr_arr[idx]\n",
    "    prec_arr, rec_arr, _ = precision_recall_curve(y_true, y_proba)\n",
    "    prec_val = prec_arr[np.argmin(np.abs(rec_arr - recall_val))]\n",
    "    return recall_val, prec_val, threshold, actual_fpr\n",
    "\n",
    "TARGET_FPR = 0.01\n",
    "tuned_rec_fpr, tuned_prec_fpr, tuned_thresh, tuned_actual_fpr = recall_at_fpr(y_te, tuned_proba)\n",
    "\n",
    "print(f\"\\n  Recall @ {TARGET_FPR*100:.0f}% FPR : {tuned_rec_fpr:.4f}  \"\n",
    "      f\"(precision={tuned_prec_fpr:.4f}, threshold={tuned_thresh:.4f})\")\n",
    "\n",
    "\n",
    "# ── Cost evaluation ───────────────────────────────────────────────────────────\n",
    "FRAUD_LOSS = 200\n",
    "FP_COST    = 5\n",
    "\n",
    "def compute_cost(y_true, y_pred):\n",
    "    tn_, fp_, fn_, tp_ = confusion_matrix(y_true, y_pred).ravel()\n",
    "    return (fn_ * FRAUD_LOSS) + (fp_ * FP_COST), fn_, fp_\n",
    "\n",
    "# Optimal threshold sweep on test set\n",
    "thresholds  = np.linspace(0.01, 0.99, 300)\n",
    "tuned_costs = [compute_cost(y_te, (tuned_proba >= t).astype(int))[0] for t in thresholds]\n",
    "tuned_opt_t = thresholds[np.argmin(tuned_costs)]\n",
    "tuned_opt_cost, *_ = compute_cost(y_te, (tuned_proba >= tuned_opt_t).astype(int))\n",
    "tuned_cost_default, *_ = compute_cost(y_te, tuned_pred)\n",
    "\n",
    "print(f\"\\n  Cost @ t=0.5       : €{tuned_cost_default:,.0f}\")\n",
    "print(f\"  Cost @ t={tuned_opt_t:.2f} (opt): €{tuned_opt_cost:,.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f48f985",
   "metadata": {},
   "source": [
    "## Step 4 — Three-Way Comparison\n",
    "\n",
    "Logistic Regression and XGBoost baseline metrics are loaded from Phase 3 results.  \n",
    "XGBoost Tuned metrics are from the retrained model above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6902c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# ── Rebuild LR baseline (Phase 3 parameters, identical) ─────────────────────\n",
    "lr_pipeline = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"lr\", LogisticRegression(class_weight=\"balanced\", max_iter=1000,\n",
    "                              random_state=42, solver=\"lbfgs\", n_jobs=-1))\n",
    "])\n",
    "lr_pipeline.fit(X_train_fe, y_tr)\n",
    "lr_proba = lr_pipeline.predict_proba(X_test_fe)[:, 1]\n",
    "lr_pred  = lr_pipeline.predict(X_test_fe)\n",
    "\n",
    "# ── Rebuild XGB baseline (Phase 3 fixed params) ───────────────────────────────\n",
    "n_neg_b   = (y_tr == 0).sum()\n",
    "n_pos_b   = (y_tr == 1).sum()\n",
    "xgb_base  = XGBClassifier(\n",
    "    objective=\"binary:logistic\", eval_metric=\"aucpr\",\n",
    "    scale_pos_weight=n_neg_b/n_pos_b,\n",
    "    max_depth=6, learning_rate=0.05, n_estimators=500,\n",
    "    early_stopping_rounds=50, subsample=0.8, colsample_bytree=0.8,\n",
    "    random_state=42, n_jobs=-1, verbosity=0,\n",
    ")\n",
    "xgb_base.fit(X_tr, y_tr_,\n",
    "             eval_set=[(X_val, y_val)], verbose=False)\n",
    "xgb_proba = xgb_base.predict_proba(X_test_fe)[:, 1]\n",
    "xgb_pred  = (xgb_proba >= 0.5).astype(int)\n",
    "\n",
    "# ── Helper: all metrics for one model ────────────────────────────────────────\n",
    "def all_metrics(y_true, y_proba, y_pred, label):\n",
    "    roc  = roc_auc_score(y_true, y_proba)\n",
    "    pr   = average_precision_score(y_true, y_proba)\n",
    "    rec_fpr, prec_fpr, thr, afpr = recall_at_fpr(y_true, y_proba)\n",
    "    _, _, f1_, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred, pos_label=1, average=\"binary\")\n",
    "    costs = [compute_cost(y_true, (y_proba >= t).astype(int))[0] for t in thresholds]\n",
    "    opt_t = thresholds[np.argmin(costs)]\n",
    "    opt_c, *_ = compute_cost(y_true, (y_proba >= opt_t).astype(int))\n",
    "    def_c, *_ = compute_cost(y_true, y_pred)\n",
    "    return {\n",
    "        \"Model\"              : label,\n",
    "        \"ROC-AUC\"            : roc,\n",
    "        \"PR-AUC\"             : pr,\n",
    "        f\"Recall@{TARGET_FPR*100:.0f}%FPR\" : rec_fpr,\n",
    "        f\"Prec@{TARGET_FPR*100:.0f}%FPR\"   : prec_fpr,\n",
    "        \"F1 (t=0.5)\"         : f1_,\n",
    "        \"Cost € (t=0.5)\"     : def_c,\n",
    "        \"Cost € (opt t)\"     : opt_c,\n",
    "        \"Opt threshold\"      : opt_t,\n",
    "    }\n",
    "\n",
    "rows = [\n",
    "    all_metrics(y_te, lr_proba,    lr_pred,    \"Logistic Regression\"),\n",
    "    all_metrics(y_te, xgb_proba,   xgb_pred,   \"XGBoost Baseline\"),\n",
    "    all_metrics(y_te, tuned_proba, tuned_pred,  \"XGBoost Tuned\"),\n",
    "]\n",
    "results = pd.DataFrame(rows).set_index(\"Model\")\n",
    "\n",
    "# ── Print table ───────────────────────────────────────────────────────────────\n",
    "fmt = {\n",
    "    \"ROC-AUC\"                          : \"{:.4f}\",\n",
    "    \"PR-AUC\"                           : \"{:.4f}\",\n",
    "    f\"Recall@{TARGET_FPR*100:.0f}%FPR\" : \"{:.4f}\",\n",
    "    f\"Prec@{TARGET_FPR*100:.0f}%FPR\"   : \"{:.4f}\",\n",
    "    \"F1 (t=0.5)\"                       : \"{:.4f}\",\n",
    "    \"Cost € (t=0.5)\"                   : \"€{:,.0f}\",\n",
    "    \"Cost € (opt t)\"                   : \"€{:,.0f}\",\n",
    "    \"Opt threshold\"                    : \"{:.2f}\",\n",
    "}\n",
    "display_df = results.copy()\n",
    "for col, f in fmt.items():\n",
    "    display_df[col] = display_df[col].map(lambda v, f=f: f.format(v))\n",
    "\n",
    "print(\"=\" * 90)\n",
    "print(display_df.to_string())\n",
    "print(\"=\" * 90)\n",
    "print(f\"\\nCost assumptions: FN × €{FRAUD_LOSS}  +  FP × €{FP_COST}\")\n",
    "print(f\"Best Optuna val PR-AUC: {best.value:.6f}  (trial #{best.number}, {N_TRIALS} total trials)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f330be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Three-way ROC + PR curve plot ────────────────────────────────────────────\n",
    "models_plot = [\n",
    "    (\"Logistic Regression\", lr_proba,    \"royalblue\"),\n",
    "    (\"XGBoost Baseline\",    xgb_proba,   \"steelblue\"),\n",
    "    (\"XGBoost Tuned\",       tuned_proba, \"crimson\"),\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "for label, proba, color in models_plot:\n",
    "    fpr_, tpr_, _ = roc_curve(y_te, proba)\n",
    "    auc = roc_auc_score(y_te, proba)\n",
    "    axes[0].plot(fpr_, tpr_, label=f\"{label}  AUC={auc:.4f}\", color=color, lw=2)\n",
    "\n",
    "axes[0].axvline(TARGET_FPR, color=\"gray\", linestyle=\"--\", lw=1,\n",
    "                label=f\"FPR={TARGET_FPR}\")\n",
    "axes[0].plot([0,1],[0,1], \"k--\", lw=0.8, alpha=0.4)\n",
    "axes[0].set_title(\"ROC Curve — Three-Way Comparison\", fontweight=\"bold\")\n",
    "axes[0].set_xlabel(\"False Positive Rate\")\n",
    "axes[0].set_ylabel(\"True Positive Rate\")\n",
    "axes[0].legend(fontsize=9)\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "for label, proba, color in models_plot:\n",
    "    p_, r_, _ = precision_recall_curve(y_te, proba)\n",
    "    ap = average_precision_score(y_te, proba)\n",
    "    axes[1].plot(r_, p_, label=f\"{label}  AP={ap:.4f}\", color=color, lw=2)\n",
    "\n",
    "axes[1].axhline(y_te.mean(), color=\"gray\", linestyle=\"--\", lw=1,\n",
    "                label=f\"Baseline (fraud={y_te.mean():.4f})\")\n",
    "axes[1].set_title(\"Precision–Recall Curve — Three-Way Comparison\", fontweight=\"bold\")\n",
    "axes[1].set_xlabel(\"Recall\")\n",
    "axes[1].set_ylabel(\"Precision\")\n",
    "axes[1].legend(fontsize=9)\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"data/processed/comparison_roc_pr.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "print(\"Saved: data/processed/comparison_roc_pr.png\")\n",
    "print(\"\\nPhase 4 complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
